{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e958d0a0",
   "metadata": {},
   "source": [
    "\n",
    "# IsolationForest Anomaly Detection â€” Notebook\n",
    "\n",
    "This notebook shows how to **train** and **run inference** using your project modules:\n",
    "- `src/pipelines.py`\n",
    "- `src/utils.py`\n",
    "- `src/visualize.py` (optional plots)\n",
    "\n",
    "> Run this notebook from the **project root** so relative paths resolve (e.g., `configs/config.yaml`, `data/train.csv`).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84da280e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# (Optional) Install dependencies if needed in your environment\n",
    "# %pip install -r requirements.txt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f998f3a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "\n",
    "# Ensure project root is on sys.path (assumes the notebook is at project root)\n",
    "project_root = Path.cwd()\n",
    "if str(project_root) not in sys.path:\n",
    "    sys.path.append(str(project_root))\n",
    "\n",
    "from src.pipelines import build_pipeline\n",
    "from src.utils import load_config, ensure_features, default_meta, save_meta, annotate_with_iforest\n",
    "from joblib import dump, load\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ad534a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load configuration\n",
    "cfg_path = Path(\"configs/config.yaml\")\n",
    "assert cfg_path.exists(), \"configs/config.yaml not found. Please create it or adjust the path.\"\n",
    "\n",
    "cfg = load_config(cfg_path)\n",
    "cfg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22a126c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Train ===\n",
    "features = cfg[\"features\"]\n",
    "contamination = float(cfg.get(\"contamination\", 0.1))\n",
    "random_state = int(cfg.get(\"random_state\", 42))\n",
    "imputer_strategy = cfg.get(\"imputer_strategy\", \"median\")\n",
    "scaler = cfg.get(\"scaler\", \"standard\")\n",
    "\n",
    "train_csv = Path(\"data/train.csv\")\n",
    "assert train_csv.exists(), \"data/train.csv not found. Place your training CSV there or change the path.\"\n",
    "\n",
    "df_train = pd.read_csv(train_csv)\n",
    "ensure_features(df_train, features)\n",
    "\n",
    "pipe = build_pipeline(\n",
    "    features=features,\n",
    "    contamination=contamination,\n",
    "    random_state=random_state,\n",
    "    imputer_strategy=imputer_strategy,\n",
    "    scaler=scaler,\n",
    ")\n",
    "\n",
    "pipe.fit(df_train)\n",
    "\n",
    "model_path = Path(cfg.get(\"model_path\", \"models/iforest_pipeline.joblib\"))\n",
    "meta_path  = Path(cfg.get(\"meta_path\", \"models/iforest_meta.json\"))\n",
    "model_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "dump(pipe, model_path)\n",
    "save_meta(default_meta(cfg, model_path), meta_path)\n",
    "\n",
    "print(\"âœ… Trained and saved:\", model_path)\n",
    "print(\"ðŸ“ Meta saved:\", meta_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35e9a99b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Inference ===\n",
    "pipe_loaded = load(model_path)\n",
    "\n",
    "test_csv = Path(\"data/new_data.csv\")\n",
    "assert test_csv.exists(), \"data/new_data.csv not found. Place your scoring CSV there or change the path.\"\n",
    "\n",
    "df_new = pd.read_csv(test_csv)\n",
    "ensure_features(df_new, features)\n",
    "\n",
    "df_annot = annotate_with_iforest(df_new, pipe_loaded, features)\n",
    "df_annot.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c64c1b4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# === Visualizations (Plotly) ===\n",
    "import plotly.express as px\n",
    "from src.visualize import (\n",
    "    plot_scatter_melted,\n",
    "    plot_score_hist,\n",
    "    plot_pair_scatter,\n",
    "    plot_feature_box,\n",
    ")\n",
    "\n",
    "# Combined melted scatter (index vs values), colored by variable, anomalies as symbols\n",
    "fig1 = plot_scatter_melted(df_annot, features, show=True)\n",
    "\n",
    "# Score distribution\n",
    "fig2 = plot_score_hist(df_annot, show=True)\n",
    "\n",
    "# Pairwise feature scatter (e.g., RHOB vs NPHI)\n",
    "if len(features) >= 2:\n",
    "    fig3 = plot_pair_scatter(df_annot, x=features[1], y=features[0], show=True)\n",
    "\n",
    "# Box plots (melted)\n",
    "fig4 = plot_feature_box(df_annot, features, points=\"all\", show=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc8e2c69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save scored/annotated data from notebook\n",
    "out_csv = Path(\"scored_from_notebook.csv\")\n",
    "df_annot.to_csv(out_csv, index=False)\n",
    "print(\"ðŸ’¾ Wrote:\", out_csv.resolve())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f2fa2d7",
   "metadata": {},
   "source": [
    "\n",
    "## Run CLI scripts from the notebook (alternative)\n",
    "If you prefer, you can call the existing scripts directly:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31c0ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# !python scripts/train.py --data data/train.csv --config configs/config.yaml\n",
    "# !python scripts/inference.py --data data/new_data.csv --config configs/config.yaml --out scored.csv --plot\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
